{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train your DragoNN primer tutorial\n",
    "\n",
    "This tutorial is a supplement to the DragoNN manuscript.\n",
    "\n",
    "## Outline\n",
    "    * How to use this tutorial\n",
    "    * Review of patterns in transcription factor binding sites\n",
    "    * Learning to localize homotypic motif density\n",
    "    * Sequence model definition\n",
    "    * Evaluation and visualization of\n",
    "        - single layer, single filter DragoNN\n",
    "        - single layer, multiple filters DragoNN\n",
    "        - Multi-layer DragoNN\n",
    "        - Regularized multi-layer DragoNN\n",
    "\n",
    "Github issues on the dragonn repository with feedback, questions, and discussion are always welcome.\n",
    "\n",
    "\n",
    "## How to use this tutorial\n",
    "\n",
    "This tutorial utilizes a Jupyter/IPython Notebook - an interactive computational enviroment that combines live code, visualizations, and explanatory text. The notebook is organized into a series of cells. You can run the next cell by cliking the play button:\n",
    "![play button](./primer_tutorial_images/play_button.png)\n",
    "You can also run all cells in a series by clicking \"run all\" in the Cell drop-down menu:\n",
    "![play all button](./primer_tutorial_images/play_all_button.png)\n",
    "Half of the cells in this tutorial contain code, the other half contain visualizations and explanatory text. Code, visualizations, and text in cells can be modified - you are encouraged to modify the code as you advance through the tutorial. You can inspect the implementation of a function used in a cell by following these steps:\n",
    "![inspecting code](./primer_tutorial_images/inspecting_code.png)\n",
    "\n",
    "We start by loading dragonn's tutorial utilities and reviewing properties of regulatory sequence that transcription factors bind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import pickle\n",
    "from dragonn.tutorial_utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sequence properties 1](./primer_tutorial_images/sequence_properties_1.jpg)\n",
    "![sequence properties 2](./primer_tutorial_images/sequence_properties_2.jpg)\n",
    "\n",
    "# Learning to localize a homotypic motif density\n",
    "In this tutorial we will learn how to localize a homotypic motif cluster. We will simulate a positive set of sequences with multiple instances of a motif in the center and a negative set of sequences with multiple motif instances positioned anywhere in the sequence:\n",
    "![homotypic motif density localization](./primer_tutorial_images/homotypic_motif_density_localization.jpg)\n",
    "We will then train a binary classification model to classify the simulated sequences. To solve this task, the model will need to learn the motif pattern and whether instances of that pattern are present in the central part of the sequence.\n",
    "\n",
    "We start by getting the simulation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting simulation data\n",
    "\n",
    "DragoNN provides a set of simulation functions. We will use the simulate_motif_density_localization function to simulate homotypic motif density localization. First, we obtain documentation for the simulation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_simulation_info(\"simulate_motif_density_localization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define parameters for a TAL1 motif density localization in 1500bp long sequence, with 0.4 GC fraction, and 2-4 instances of the motif in the central 150bp for the positive sequences. We simulate a total of 3000 positive and 3000 negative sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_density_localization_simulation_parameters = {\n",
    "    \"motif_name\": \"TAL1_known4\",\n",
    "    \"seq_length\": 1000,\n",
    "    \"center_size\": 150,\n",
    "    \"min_motif_counts\": 2,\n",
    "    \"max_motif_counts\": 4, \n",
    "    \"num_pos\": 3000,\n",
    "    \"num_neg\": 3000,\n",
    "    \"GC_fraction\": 0.4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the simulation data by calling the get_simulation_data function with the simulation name and the simulation parameters as inputs. 1000 sequences are held out for a test set, 1000 sequences for a validation set, and the remaining 4000 sequences are in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_data = get_simulation_data(\"simulate_motif_density_localization\",\n",
    "                                      motif_density_localization_simulation_parameters,\n",
    "                                      validation_set_size=1000, test_set_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simulation_data provides training, validation, and test sets of input sequences X and sequence labels y. The inputs X are matrices with a one-hot-encoding of the sequences:\n",
    "<img src=\"tutorial_images/one_hot_encoding.png\" width=\"500\">\n",
    "\n",
    "Here are the first 10bp of a sequence in our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_data.X_train[0, :, :, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matrix represent the 10bp sequence AAATGGGCCG.\n",
    "\n",
    "# The homotypic motif density localization task\n",
    "The goal of the model is to take the positive and negative sequences simulated above and classify them:\n",
    "![classificatioin task](./tutorial_images/homotypic_motif_density_localization_task.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DragoNN Models\n",
    "\n",
    "A locally connected linear unit in a DragoNN model can represent a PSSM (part a). A sequence PSSM score is obtained by multiplying the PSSM across the sequence, thersholding the PSSM scores, and taking the max (part b). A PSSM score can also be computed by a DragoNN model with tiled locally connected linear units, amounting to a convolutional layer with a single convolutional filter representing the PSSM, followed by ReLU thersholding and maxpooling (part c).\n",
    "![dragonn vs pssm](./tutorial_images/dragonn_and_pssm.jpg)\n",
    "By utilizing multiple convolutional layers with multiple convolutional filters, DragoNN models can represent a wide range of sequence features in a compositional fashion:\n",
    "![dragonn model figure](./tutorial_images/dragonn_model_figure.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting a DragoNN model\n",
    "\n",
    "The main DragoNN model class is SequenceDNN, which provides a simple interface to a range of models and methods to train, test, and interpret DragoNNs. SequenceDNN uses [keras](http://keras.io/), a deep learning library for [Theano](https://github.com/Theano/Theano) and [TensorFlow](https://github.com/tensorflow/tensorflow), which are popular software packages for deep learning.\n",
    "\n",
    "We will inspect the SequenceDNN class, look at examples of DragoNN architecture definitions, and analyze trained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a description of the architecture parameters we use the inspect_SequenceDNN function, which outputs documentation for the model class including the architecture parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_SequenceDNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Available methods\" display what can be done with a SequenceDNN model. These include common operations such as training and testing the model, and more complex operations such as extracting insight from trained models. We define a simple DragoNN model with one convolutional layer with one convolutional filter, followed by maxpooling of width 35. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_filter_dragonn_parameters = {\n",
    "    'seq_length': 1000,\n",
    "    'num_filters': [1],\n",
    "    'conv_width': [10],\n",
    "    'num_epochs': 150,\n",
    "    'pool_width': 35}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can get a radnomly initialized DragoNN model by calling the get_SequenceDNN function with one_filter_dragonn_parameters as the input: one_filter_dragonn = get_SequenceDNN(one_filter_dragonn_parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a DragoNN model\n",
    "\n",
    "Next, we evaluate the one_filter_dragonn by loading an already trained model and its training history. In each epoch, the one_filter_dragonn performed a complete pass over the training data, and updated its parameters to minimize the loss, which quantifies the error in the model predictions. After each epoch, the performance metrics for the one_filter_dragonn on the validation data were stored. Training stopped once the loss on the validation stopped improving for multiple consecutive epochs. The performance metrics include balanced accuracy, area under the receiver-operating curve ([auROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)), are under the precision-recall curve ([auPRC](https://en.wikipedia.org/wiki/Precision_and_recall)), and recall for multiple false discovery rates  (Recall at [FDR](https://en.wikipedia.org/wiki/False_discovery_rate))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "one_filter_dragonn = SequenceDNN.load(\"primer_tutorial_models/one_filter_dragonn.arch.json\",\n",
    "                                           \"primer_tutorial_models/one_filter_dragonn.weights.h5\")\n",
    "with open('primer_tutorial_models/one_filter_dragonn.train_metrics.pkl', 'rb') as handle:\n",
    "    one_filter_dragonn.train_metrics = pickle.load(handle,encoding=\"latin1\")\n",
    "with open('primer_tutorial_models/one_filter_dragonn.valid_metrics.pkl', 'rb') as handle:\n",
    "    one_filter_dragonn.valid_metrics = pickle.load(handle,encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training and validation loss history during training, and performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SequenceDNN_learning_curve(one_filter_dragonn)\n",
    "test_SequenceDNN(one_filter_dragonn, simulation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single layer, single filter model gets good performance and doesn't overfit much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's visualize the filter learned in this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_SequenceDNN_filters(one_filter_dragonn, simulation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filter captures the reverse complemnt of the simulated motif."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A multi-filter DragoNN model \n",
    "Next, we modify the model to have 15 convolutional filters instead of just one filter. How does this model compare to the single filter model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_filter_dragonn_parameters = {\n",
    "    'seq_length': 1000,\n",
    "    'num_filters': [15], ## notice the change from 1 filter to 15 filters\n",
    "    'conv_width': [10],\n",
    "    'pool_width': 35}\n",
    "multi_filter_dragonn = SequenceDNN.load(\"primer_tutorial_models/multi_filter_dragonn.arch.json\",\n",
    "                                           \"primer_tutorial_models/multi_filter_dragonn.weights.h5\")\n",
    "with open('primer_tutorial_models/multi_filter_dragonn.train_metrics.pkl', 'rb') as handle:\n",
    "    multi_filter_dragonn.train_metrics = pickle.load(handle,encoding=\"latin1\")\n",
    "with open('primer_tutorial_models/multi_filter_dragonn.valid_metrics.pkl', 'rb') as handle:\n",
    "    multi_filter_dragonn.valid_metrics = pickle.load(handle,encoding=\"latin1\")\n",
    "SequenceDNN_learning_curve(multi_filter_dragonn)\n",
    "test_SequenceDNN(multi_filter_dragonn, simulation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It slightly outperforms the single filter model and exhibits some overfitting. Let's check if the learned filters capture the simulated pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_SequenceDNN_filters(multi_filter_dragonn, simulation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only some of the filters closesly match the simulated pattern. This illustrates that interpreting model parameters directly works partially for multi-filter models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A multi-layer DragoNN model\n",
    "Next, we train a 3 layer model for this task. Will it outperform the single layer model and to what extent will it overfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_layer_dragonn_parameters = {\n",
    "    'seq_length': 1000,\n",
    "    'num_filters': [15, 15, 15], ## notice the change to multiple filter values, one for each layer\n",
    "    'conv_width': [10, 10, 10],\n",
    "    'pool_width': 35}\n",
    "multi_layer_dragonn = SequenceDNN.load(\"primer_tutorial_models/multi_layer_dragonn.arch.json\",\n",
    "                                           \"primer_tutorial_models/multi_layer_dragonn.weights.h5\")\n",
    "with open('primer_tutorial_models/multi_layer_dragonn.train_metrics.pkl', 'rb') as handle:\n",
    "    multi_layer_dragonn.train_metrics = pickle.load(handle,encoding=\"latin1\")\n",
    "with open('primer_tutorial_models/multi_layer_dragonn.valid_metrics.pkl', 'rb') as handle:\n",
    "    multi_layer_dragonn.valid_metrics = pickle.load(handle,encoding=\"latin1\")\n",
    "SequenceDNN_learning_curve(multi_layer_dragonn)\n",
    "test_SequenceDNN(multi_layer_dragonn, simulation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs slightly better than the single layer model but it overfits more. We will try to address that with dropout regularization. But first, what do the first layer filters look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_SequenceDNN_filters(multi_layer_dragonn, simulation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filters now make less sense than in the single layer model case. In multi-layered models, sequence features are learned compositionally across the layers. As a result, sequence filters in the first layer focus more on simple features that can be combined in higher layers to learn motif features more efficiently, and their interpretation becomes less clear based on simple visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A regularized multi-layer DragoNN model\n",
    "Next, we regularize the 3 layer using 0.2 dropout on every convolutional layer. Will dropout improve validation performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regularized_multi_layer_dragonn_parameters = {\n",
    "    'seq_length': 1000,\n",
    "    'num_filters': [15, 15, 15],\n",
    "    'conv_width': [10, 10, 10],\n",
    "    'pool_width': 35,\n",
    "    'dropout': 0.2} ## we introduce dropout of 0.2 on every convolutional layer for regularization\n",
    "regularized_multi_layer_dragonn = SequenceDNN.load(\n",
    "    \"primer_tutorial_models/regularized_multi_layer_dragonn.arch.json\",\n",
    "    \"primer_tutorial_models/regularized_multi_layer_dragonn.weights.h5\")\n",
    "with open('primer_tutorial_models/regularized_multi_layer_dragonn.train_metrics.pkl', 'rb') as handle:\n",
    "    regularized_multi_layer_dragonn.train_metrics = pickle.load(handle,encoding=\"latin1\")\n",
    "with open('primer_tutorial_models/regularized_multi_layer_dragonn.valid_metrics.pkl', 'rb') as handle:\n",
    "    regularized_multi_layer_dragonn.valid_metrics = pickle.load(handle,encoding=\"latin1\")\n",
    "SequenceDNN_learning_curve(regularized_multi_layer_dragonn)\n",
    "test_SequenceDNN(regularized_multi_layer_dragonn, simulation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, dropout decreased the overfitting this model displayed previously and increased test performance. Let's see the learned filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_SequenceDNN_filters(regularized_multi_layer_dragonn, simulation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous multi layer model, individual filters do not capture the simulated motif."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
