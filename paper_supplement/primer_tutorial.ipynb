{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional neural networks for genomics: \n",
    "## How to train your DragoNN tutorial\n",
    "\n",
    "This tutorial is a supplement to the DragoNN manuscript.\n",
    "\n",
    "## Outline<a name='outline'>\n",
    "<ol>\n",
    "    <li><a href=#1>How to use this tutorial</a></li>\n",
    "    <li><a href=#2>Review of patterns in transcription factor binding sites</a></li>\n",
    "    <li><a href=#3>Learning to localize homotypic motif density</a></li>\n",
    "    <li><a href=#4>Simulate training data with simdna</a></li>\n",
    "    \n",
    "[The DragoNN software package](https://github.com/kundajelab/dragonn) provides a number of utilities for training convolutional neural networks (CNN's) for genomic sequences. We illustrate how DragoNN can be used to train and interpret a series of increasingly complex CNN's. For more advanced users, we provide the corresponding [keras](http://keras.io) code. \n",
    "\n",
    "    \n",
    "   <li><a href=#5>Model architecture</a></li>\n",
    "   <li><a href=#6>Model training</a></li>\n",
    "   <li><a href=#7>Model interpretation</a></li>\n",
    "   <li><a href=#8>single layer, multiple filter models</a></li>\n",
    "            <ol>\n",
    "                <li><a href=#8a>DragoNN implementation</a></li>\n",
    "                <li><a href=#8b>Keras implementation</a></li>\n",
    "            </ol>\n",
    "            <li><a href=#9>Multi-layer models</a></li>\n",
    "            <ol>\n",
    "                <li><a href=#9a>DragoNN implementation</a></li>\n",
    "                <li><a href=#9b>Keras implementation</a></li>\n",
    "            </ol>\n",
    "            <li><a href=#10>Regularized multi-layer models</a></li>\n",
    "            <ol>\n",
    "                <li><a href=#10a>DragoNN implementation</a></li>\n",
    "                <li><a href=#10b>Keras implementation</a></li>\n",
    "            </ol>\n",
    "    <li><a href=#11>Further exploration</a></li>\n",
    "    <li><a href=#12>Using DragoNN with your own non-simulated data</a></li>\n",
    "</ol>\n",
    "Github issues on the dragonn repository with feedback, questions, and discussion are always welcome.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use this tutorial<a name='1'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "This tutorial utilizes a Jupyter/IPython Notebook - an interactive computational enviroment that combines live code, visualizations, and explanatory text. The notebook is organized into a series of cells. You can run the next cell by cliking the play button:\n",
    "![play button](./primer_tutorial_images/play_button.png)\n",
    "You can also run all cells in a series by clicking \"run all\" in the Cell drop-down menu:\n",
    "![play all button](./primer_tutorial_images/play_all_button.png)\n",
    "Half of the cells in this tutorial contain code, the other half contain visualizations and explanatory text. Code, visualizations, and text in cells can be modified - you are encouraged to modify the code as you advance through the tutorial. You can inspect the implementation of a function used in a cell by following these steps:\n",
    "![inspecting code](./primer_tutorial_images/inspecting_code.png)\n",
    "\n",
    "We start by loading dragonn's tutorial utilities and reviewing properties of regulatory sequence that transcription factors bind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dragonn tutorial utilities \n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from dragonn.tutorial_utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key properties of regulatory DNA sequences <a name='2'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "![sequence properties 1](./primer_tutorial_images/sequence_properties_1.jpg)\n",
    "![sequence properties 2](./primer_tutorial_images/sequence_properties_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning to localize homotypic motif density <a name='3'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "In this tutorial we will learn how to localize a homotypic motif cluster. We will simulate a positive set of sequences with multiple instances of a motif in the center and a negative set of sequences with multiple motif instances positioned anywhere in the sequence:\n",
    "![homotypic motif density localization](./primer_tutorial_images/homotypic_motif_density_localization.jpg)\n",
    "We will then train a binary classification model to classify the simulated sequences. To solve this task, the model will need to learn the motif pattern and whether instances of that pattern are present in the central part of the sequence.\n",
    "\n",
    "![classificatioin task](./tutorial_images/homotypic_motif_density_localization_task.jpg)\n",
    "\n",
    "We start by getting the simulation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting simulation data <a name='4'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "\n",
    "DragoNN provides a set of simulation functions. We will use the **simulate_motif_density_localization** function to simulate homotypic motif density localization. First, we obtain documentation for the simulation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Simulates two classes of seqeuences:\n",
      "        - Positive class sequences with multiple motif instances\n",
      "          in center of the sequence.\n",
      "        - Negative class sequences with multiple motif instances\n",
      "          anywhere in the sequence.\n",
      "    The number of motif instances is uniformly sampled\n",
      "    between minimum and maximum motif counts.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    motif_name : str\n",
      "        encode motif name\n",
      "    seq_length : int\n",
      "        length of sequence\n",
      "    center_size : int\n",
      "        length of central part of the sequence where motifs can be positioned\n",
      "    min_motif_counts : int\n",
      "        minimum number of motif instances\n",
      "    max_motif_counts : int\n",
      "        maximum number of motif instances\n",
      "    num_pos : int\n",
      "        number of positive class sequences\n",
      "    num_neg : int\n",
      "        number of negative class sequences\n",
      "    GC_fraction : float\n",
      "        GC fraction in background sequence\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    sequence_arr : 1darray\n",
      "        Contains sequence strings.\n",
      "    y : 1darray\n",
      "        Contains labels.\n",
      "    embedding_arr: 1darray\n",
      "        Array of embedding objects.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print_simulation_info(\"simulate_motif_density_localization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define parameters for a TAL1 motif density localization in 1500bp long sequence, with 0.4 GC fraction, and 2-4 instances of the motif in the central 150bp for the positive sequences. We simulate a total of 3000 positive and 3000 negative sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "motif_density_localization_simulation_parameters = {\n",
    "    \"motif_name\": \"TAL1_known4\",\n",
    "    \"seq_length\": 1500,\n",
    "    \"center_size\": 150,\n",
    "    \"min_motif_counts\": 2,\n",
    "    \"max_motif_counts\": 4, \n",
    "    \"num_pos\": 3000,\n",
    "    \"num_neg\": 3000,\n",
    "    \"GC_fraction\": 0.4}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the simulation data by calling the **get_simulation_data** function with the simulation name and the simulation parameters as inputs. 1000 sequences are held out for a test set, 1000 sequences for a validation set, and the remaining 4000 sequences are in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_data = get_simulation_data(\"simulate_motif_density_localization\",\n",
    "                                      motif_density_localization_simulation_parameters,\n",
    "                                      validation_set_size=1000, test_set_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simulation_data provides training, validation, and test sets of input sequences X and sequence labels y. The inputs X are matrices with a one-hot-encoding of the sequences:\n",
    "<img src=\"tutorial_images/one_hot_encoding.png\" width=\"500\">\n",
    "\n",
    "Here are the first 10bp of a sequence in our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, 0, 1, 1, 0, 1, 0, 1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 1, 1, 0, 0, 1, 0, 1, 0]]], dtype=int32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulation_data.X_train[0, :, :, :10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert this one-hot-encoded matrix back into a DNA string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'GTTTAATATA'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dragonn.utils import *\n",
    "get_sequence_strings(simulation_data.X_train)[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the shape of training, validation, and test matrices: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 1, 4, 1000)\n",
      "(4000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(simulation_data.X_train.shape)\n",
    "print(simulation_data.y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1, 4, 1000)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(simulation_data.X_valid.shape)\n",
    "print(simulation_data.y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1, 4, 1000)\n",
      "(1000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(simulation_data.X_test.shape)\n",
    "print(simulation_data.y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the convolutional neural network model architecture  <a name='5'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "A locally connected linear unit in a CNN model can represent a PSSM (part a). A sequence PSSM score is obtained by multiplying the PSSM across the sequence, thresholding the PSSM scores, and taking the max (part b). A PSSM score can also be computed by a CNN model with tiled, locally connected linear units, amounting to a convolutional layer with a single convolutional filter representing the PSSM, followed by ReLU thresholding and maxpooling (part c).\n",
    "![dragonn vs pssm](./tutorial_images/dragonn_and_pssm.jpg)\n",
    "By utilizing multiple convolutional layers with multiple convolutional filters, CNN's can represent a wide range of sequence features in a compositional fashion:\n",
    "![dragonn model figure](./tutorial_images/dragonn_model_figure.jpg)\n",
    "\n",
    "The main DragoNN model class, **SequenceDNN**, provides a simple interface to a range of models and methods to train, test, and interpret CNN's for genomic data. SequenceDNN uses [keras](http://keras.io/), a deep learning library for [TensorFlow](https://github.com/tensorflow/tensorflow),a popular software package for deep learning.\n",
    "\n",
    "We will train, test, and interpret a series of increasingly complex CNN models via the DragoNN interface.\n",
    "For users experienced with keras, each DragoNN vignette is accompanied by the equivalent keras code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a description of the architecture parameters we use the inspect_SequenceDNN function, which outputs documentation for the model class including the architecture parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence DNN models.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "seq_length : int, optional\n",
      "    length of input sequence.\n",
      "keras_model : instance of keras.models.Sequential, optional\n",
      "    seq_length or keras_model must be specified.\n",
      "num_tasks : int, optional\n",
      "    number of tasks. Default: 1.\n",
      "num_filters : list[int] | tuple[int]\n",
      "    number of convolutional filters in each layer. Default: (15,).\n",
      "conv_width : list[int] | tuple[int]\n",
      "    width of each layer's convolutional filters. Default: (15,).\n",
      "pool_width : int\n",
      "    width of max pooling after the last layer. Default: 35.\n",
      "L1 : float\n",
      "    strength of L1 penalty.\n",
      "dropout : float\n",
      "    dropout probability in every convolutional layer. Default: 0.\n",
      "verbose: int\n",
      "    Verbosity level during training. Valida values: 0, 1, 2.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "Compiled DNN model.\n",
      "\n",
      "Available methods:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inspect_SequenceDNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Available methods\" display what can be done with a SequenceDNN model. These include common operations such as training and testing the model, and more complex operations such as extracting insight from trained models. We define a simple DragoNN model with one convolutional layer with one convolutional filter, followed by maxpooling of width 35. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_filter_dragonn_parameters = {\n",
    "    'seq_length': 1000,\n",
    "    'num_filters': [1],\n",
    "    'conv_width': [10],\n",
    "    'num_epochs': 150,\n",
    "    'pool_width': 35}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can get a radnomly initialized DragoNN model by calling the **get_SequenceDNN** function with **one_filter_dragonn_parameters** as the input: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TimeDistributedDense'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-908edb8b3a44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mone_filter_dragonn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_SequenceDNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_filter_dragonn_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/dragonn-0.1.3-py3.6.egg/dragonn/tutorial_utils.py\u001b[0m in \u001b[0;36mget_SequenceDNN\u001b[0;34m(SequenceDNN_parameters)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_SequenceDNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSequenceDNN_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mSequenceDNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mSequenceDNN_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/dragonn-0.1.3-py3.6.egg/dragonn/models.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, seq_length, keras_model, use_RNN, num_tasks, num_filters, conv_width, pool_width, GRU_size, TDD_size, L1, dropout, num_epochs, verbose)\u001b[0m\n\u001b[1;32m     73\u001b[0m                  L1=0, dropout=0.0, num_epochs=100, verbose=1):\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         from keras.layers.core import (\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mPermute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTimeDistributedDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TimeDistributedDense'"
     ]
    }
   ],
   "source": [
    "one_filter_dragonn = get_SequenceDNN(one_filter_dragonn_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The corresponding keras operations are presented below -- if you are not familiar with keras, please continue to <a href=#6>Model Training</a>**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary functions and submodules from keras\n",
    "from keras.models import Sequential\n",
    "import keras.layers as kl\n",
    "from keras.callbacks import EarlyStopping, History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model architecture in keras, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training <a name='6'>\n",
    "\n",
    "Next, we evaluate the one_filter_dragonn by loading an already trained model and its training history. In each epoch, the one_filter_dragonn performed a complete pass over the training data, and updated its parameters to minimize the loss, which quantifies the error in the model predictions. After each epoch, the performance metrics for the one_filter_dragonn on the validation data were stored. Training stopped once the loss on the validation stopped improving for multiple consecutive epochs. The performance metrics include balanced accuracy, area under the receiver-operating curve ([auROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)), are under the precision-recall curve ([auPRC](https://en.wikipedia.org/wiki/Precision_and_recall)), and recall for multiple false discovery rates  (Recall at [FDR](https://en.wikipedia.org/wiki/False_discovery_rate))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "one_filter_dragonn = SequenceDNN.load(\"primer_tutorial_models/one_filter_dragonn.arch.json\",\n",
    "                                           \"primer_tutorial_models/one_filter_dragonn.weights.h5\")\n",
    "with open('primer_tutorial_models/one_filter_dragonn.train_metrics.pkl', 'rb') as handle:\n",
    "    one_filter_dragonn.train_metrics = pickle.load(handle,encoding=\"latin1\")\n",
    "with open('primer_tutorial_models/one_filter_dragonn.valid_metrics.pkl', 'rb') as handle:\n",
    "    one_filter_dragonn.valid_metrics = pickle.load(handle,encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training and validation loss history during training, and performance on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SequenceDNN_learning_curve(one_filter_dragonn)\n",
    "test_SequenceDNN(one_filter_dragonn, simulation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single layer, single filter model gets good performance and doesn't overfit much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's visualize the filter learned in this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_SequenceDNN_filters(one_filter_dragonn, simulation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filter captures the reverse complemnt of the simulated motif."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A multi-filter DragoNN model <a name='7'>\n",
    "Next, we modify the model to have 15 convolutional filters instead of just one filter. How does this model compare to the single filter model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_filter_dragonn_parameters = {\n",
    "    'seq_length': 1000,\n",
    "    'num_filters': [15], ## notice the change from 1 filter to 15 filters\n",
    "    'conv_width': [10],\n",
    "    'pool_width': 35}\n",
    "multi_filter_dragonn = SequenceDNN.load(\"primer_tutorial_models/multi_filter_dragonn.arch.json\",\n",
    "                                           \"primer_tutorial_models/multi_filter_dragonn.weights.h5\")\n",
    "with open('primer_tutorial_models/multi_filter_dragonn.train_metrics.pkl', 'rb') as handle:\n",
    "    multi_filter_dragonn.train_metrics = pickle.load(handle,encoding=\"latin1\")\n",
    "with open('primer_tutorial_models/multi_filter_dragonn.valid_metrics.pkl', 'rb') as handle:\n",
    "    multi_filter_dragonn.valid_metrics = pickle.load(handle,encoding=\"latin1\")\n",
    "SequenceDNN_learning_curve(multi_filter_dragonn)\n",
    "test_SequenceDNN(multi_filter_dragonn, simulation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It slightly outperforms the single filter model and exhibits some overfitting. Let's check if the learned filters capture the simulated pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_SequenceDNN_filters(multi_filter_dragonn, simulation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only some of the filters closesly match the simulated pattern. This illustrates that interpreting model parameters directly works partially for multi-filter models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A multi-layer DragoNN model <a name='8'>\n",
    "Next, we train a 3 layer model for this task. Will it outperform the single layer model and to what extent will it overfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_layer_dragonn_parameters = {\n",
    "    'seq_length': 1000,\n",
    "    'num_filters': [15, 15, 15], ## notice the change to multiple filter values, one for each layer\n",
    "    'conv_width': [10, 10, 10],\n",
    "    'pool_width': 35}\n",
    "multi_layer_dragonn = SequenceDNN.load(\"primer_tutorial_models/multi_layer_dragonn.arch.json\",\n",
    "                                           \"primer_tutorial_models/multi_layer_dragonn.weights.h5\")\n",
    "with open('primer_tutorial_models/multi_layer_dragonn.train_metrics.pkl', 'rb') as handle:\n",
    "    multi_layer_dragonn.train_metrics = pickle.load(handle,encoding=\"latin1\")\n",
    "with open('primer_tutorial_models/multi_layer_dragonn.valid_metrics.pkl', 'rb') as handle:\n",
    "    multi_layer_dragonn.valid_metrics = pickle.load(handle,encoding=\"latin1\")\n",
    "SequenceDNN_learning_curve(multi_layer_dragonn)\n",
    "test_SequenceDNN(multi_layer_dragonn, simulation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model performs slightly better than the single layer model but it overfits more. We will try to address that with dropout regularization. But first, what do the first layer filters look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_SequenceDNN_filters(multi_layer_dragonn, simulation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filters now make less sense than in the single layer model case. In multi-layered models, sequence features are learned compositionally across the layers. As a result, sequence filters in the first layer focus more on simple features that can be combined in higher layers to learn motif features more efficiently, and their interpretation becomes less clear based on simple visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A regularized multi-layer DragoNN model <a name='9'>\n",
    "Next, we regularize the 3 layer using 0.2 dropout on every convolutional layer. Will dropout improve validation performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regularized_multi_layer_dragonn_parameters = {\n",
    "    'seq_length': 1000,\n",
    "    'num_filters': [15, 15, 15],\n",
    "    'conv_width': [10, 10, 10],\n",
    "    'pool_width': 35,\n",
    "    'dropout': 0.2} ## we introduce dropout of 0.2 on every convolutional layer for regularization\n",
    "regularized_multi_layer_dragonn = SequenceDNN.load(\n",
    "    \"primer_tutorial_models/regularized_multi_layer_dragonn.arch.json\",\n",
    "    \"primer_tutorial_models/regularized_multi_layer_dragonn.weights.h5\")\n",
    "with open('primer_tutorial_models/regularized_multi_layer_dragonn.train_metrics.pkl', 'rb') as handle:\n",
    "    regularized_multi_layer_dragonn.train_metrics = pickle.load(handle,encoding=\"latin1\")\n",
    "with open('primer_tutorial_models/regularized_multi_layer_dragonn.valid_metrics.pkl', 'rb') as handle:\n",
    "    regularized_multi_layer_dragonn.valid_metrics = pickle.load(handle,encoding=\"latin1\")\n",
    "SequenceDNN_learning_curve(regularized_multi_layer_dragonn)\n",
    "test_SequenceDNN(regularized_multi_layer_dragonn, simulation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, dropout decreased the overfitting this model displayed previously and increased test performance. Let's see the learned filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_SequenceDNN_filters(regularized_multi_layer_dragonn, simulation_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the previous multi layer model, individual filters do not capture the simulated motif."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
