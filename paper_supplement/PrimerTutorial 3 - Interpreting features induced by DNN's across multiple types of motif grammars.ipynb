{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train your DragoNN tutorial 3: \n",
    "## Interpreting features induced by DNN's across multiple types of motif grammars \n",
    "\n",
    "This tutorial is a supplement to the DragoNN manuscript and follows figure 7 in the manuscript. \n",
    "\n",
    "This tutorial will take 1 hour  if executed on a GPU. \n",
    "\n",
    "Please complete \"Primer Tutorial 1- Exploring model architectures for a homotypic motif density simulation\" prior to completing this tutorial. \n",
    "\n",
    "The architectures used in this tutorial were determined as optimal by hyperparameter grid search in \"Primer Tutorial 3 - CNN Hyperparameter Tuning via Grid Search\"\n",
    "\n",
    "\n",
    "## Outline<a name='outline'>\n",
    "<ol>\n",
    "    <li><a href=#1>How to use this tutorial</a></li>\n",
    "    <li><a href=#2>Defining helper functions for model training and interpretation</a></li>\n",
    "    <li><a href=#3>Simulating training data with simdna: Review of Tutorial 1</a></li>\n",
    "    <li><a href=#4>Single Motif</a></li>\n",
    "    <li><a href=#5>Homotypic motif density detection</a></li>\n",
    "    <li><a href=#6>Homotypic motif density localization</a></li>\n",
    "    <li><a href=#7>Multiple motifs (multi-task)</a></li>  \n",
    "    <li><a href=#8>Heterotypic motifs spatial grammar</a></li>\n",
    "    <li><a href=#9>Conclusions</a></li>\n",
    "</ol>\n",
    "Github issues on the dragonn repository with feedback, questions, and discussion are always welcome.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use this tutorial<a name='1'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "This tutorial utilizes a Jupyter/IPython Notebook - an interactive computational enviroment that combines live code, visualizations, and explanatory text. The notebook is organized into a series of cells. You can run the next cell by cliking the play button:\n",
    "![play button](./primer_tutorial_images/play_button.png)\n",
    "You can also run all cells in a series by clicking \"run all\" in the Cell drop-down menu:\n",
    "![play all button](./primer_tutorial_images/play_all_button.png)\n",
    "Half of the cells in this tutorial contain code, the other half contain visualizations and explanatory text. Code, visualizations, and text in cells can be modified - you are encouraged to modify the code as you advance through the tutorial. You can inspect the implementation of a function used in a cell by following these steps:\n",
    "![inspecting code](./primer_tutorial_images/inspecting_code.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment the lines below if you are running this tutorial from Google Colab \n",
    "#!pip install https://github.com/kundajelab/simdna/archive/0.3.zip\n",
    "#!pip install https://github.com/kundajelab/dragonn/archive/keras_2.2_tensorflow_1.6_purekeras.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#To prepare for model training, we import the necessary functions and submodules from keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dropout, Reshape, Dense, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adadelta, SGD, RMSprop;\n",
    "import keras.losses;\n",
    "from keras.constraints import maxnorm;\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.callbacks import EarlyStopping, History\n",
    "from keras import backend as K \n",
    "K.set_image_data_format('channels_last')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading dragonn's tutorial utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dragonn tutorial utilities \n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from dragonn.tutorial_utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining helper functions for model training and interpretation  <a name='2'>\n",
    "<a href=#outline>Home</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each type of simulation, we will perform a consistent set of tasks: \n",
    "* Define the optimal model architecture, as determined in Tutorial 2 \n",
    "* Train the model on simulation data.\n",
    "* Compute the model's performance on a held-out test set.\n",
    "* Visualize the model's learning curve on training and validation data. \n",
    "* Visualize motif scores for a positive and negative example. \n",
    "* Perform in silico mutagenesis for a positive and negative example.\n",
    "* Compute DeepLIFT scores for a positive and negative example.\n",
    "\n",
    "To avoid writing the same code for each scenario, we define a series of helpers functions to perform the tasks above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(model,data):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting simulation data <a name='3'>\n",
    "<a href=#outline>Home</a>\n",
    "\n",
    "\n",
    "DragoNN provides a set of simulation functions. Let's use the **print_available_simulations** function to examine the list of simulations supported by DragoNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulate_differential_accessibility\n",
      "simulate_heterodimer_grammar\n",
      "simulate_motif_counting\n",
      "simulate_motif_density_localization\n",
      "simulate_multi_motif_embedding\n",
      "simulate_single_motif_detection\n"
     ]
    }
   ],
   "source": [
    "print_available_simulations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Motif <a name='4'>\n",
    "<a href=#outline>Home</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with single motif detection of the TAL1_known4 motif: \n",
    "\n",
    "![play button](./primer_tutorial_images/TAL1_known4.png)\n",
    "Let's find out what parameters are needed for the simulation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Simulates two classes of seqeuences:\n",
      "        - Positive class sequence with a motif\n",
      "          embedded anywhere in the sequence\n",
      "        - Negative class sequence without the motif\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    motif_name : str\n",
      "        encode motif name\n",
      "    seq_length : int\n",
      "        length of sequence\n",
      "    num_pos : int\n",
      "        number of positive class sequences\n",
      "    num_neg : int\n",
      "        number of negative class sequences\n",
      "    GC_fraction : float\n",
      "        GC fraction in background sequence\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    sequence_arr : 1darray\n",
      "        Array with sequence strings.\n",
      "    y : 1darray\n",
      "        Array with positive/negative class labels.\n",
      "    embedding_arr: 1darray\n",
      "        Array of embedding objects.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print_simulation_info(\"simulate_single_motif_detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this binary simulation task, we simulate a negative set of 10K 500 bp random sequences and a positive set of 10K 500 bp random sequences with one instance of the TAL1 motif randomly embedded at any position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_motif_simulation_parameters = {\n",
    "    \"motif_name\": \"TAL1_known4\",\n",
    "    \"seq_length\": 1500, \n",
    "    \"num_pos\": 3000,\n",
    "    \"num_neg\": 3000,\n",
    "    \"GC_fraction\": 0.4}\n",
    "tal1_data = get_simulation_data(\"simulate_single_motif_detection\",\n",
    "                                      single_motif_simulation_parameters,\n",
    "                                      validation_set_size=1000, test_set_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the convolutional neural network model architecture: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the optimal model architecture in keras (Refer to Primer Tutorial 2)\n",
    "tal1_model=Sequential() \n",
    "tal1_model.add(Conv2D(filters=5,kernel_size=(1,10),input_shape=tal1_data.X_train.shape[1::]))\n",
    "tal1_model.add(Activation('relu'))\n",
    "tal1_model.add(MaxPooling2D(pool_size=(1,10)))\n",
    "tal1_model.add(Flatten())\n",
    "tal1_model.add(Dense(1))\n",
    "tal1_model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "tal1_model.compile(optimizer='adam',\n",
    "                               loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(tal1_model,tal1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homotypic motif density detection <a name='5'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define simulation parameters \n",
    "density_detection_parameters={\n",
    "    \"motif_name\": \"TAL1_known4\",\n",
    "    \"seq_length\": 500,\n",
    "    \"neg_counts\":[0,2],\n",
    "    \"pos_counts\":[3,5],\n",
    "    \"num_pos\": 10000,\n",
    "    \"num_neg\": 10000,\n",
    "    \"GC_fraction\":0.4\n",
    "}\n",
    "\n",
    "#Get simulation data\n",
    "density_detection_data=get_simulation_data(\"simulate_motif_counting\",\n",
    "                               density_detection_parameters,\n",
    "                               validation_set_size=3200,test_set_size=4000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the optimal model architecture in keras (Refer to Primer Tutorial 2)\n",
    "density_detection_model=Sequential() \n",
    "density_detection_model.add(Conv2D(filters=3,kernel_size=(1,10),input_shape=density_detection_data.X_train.shape[1::]))\n",
    "density_detection_model.add(Activation('relu'))\n",
    "density_detection_model.add(MaxPooling2D(pool_size=(1,10)))\n",
    "density_detection_model.add(Flatten())\n",
    "density_detection_model.add(Dense(1))\n",
    "density_detection_model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "density_detection_model.compile(optimizer='adam',\n",
    "                               loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(density_detection_model,density_detection_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homotypic motif density localization <a name='6'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define simulation parameters \n",
    "density_localization_parameters = {\n",
    "    \"motif_name\": \"TAL1_known4\",\n",
    "    \"seq_length\": 1000,\n",
    "    \"center_size\": 150,\n",
    "    \"min_motif_counts\": 2,\n",
    "    \"max_motif_counts\": 4, \n",
    "    \"num_pos\": 10000,\n",
    "    \"num_neg\": 10000,\n",
    "    \"GC_fraction\": 0.4}\n",
    "\n",
    "#Get simulation data\n",
    "density_localization_data=get_simulation_data(\"simulate_motif_density_localization\",\n",
    "                               density_localization_parameters,\n",
    "                               validation_set_size=3200,test_set_size=4000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the optimal model architecture in keras (Refer to Primer Tutorial 2)\n",
    "density_localization_model=Sequential() \n",
    "density_localization_model.add(Conv2D(filters=5,kernel_size=(1,10),input_shape=density_localization_data.X_train.shape[1::]))\n",
    "density_localization_model.add(Activation('relu'))\n",
    "density_localization_model.add(MaxPooling2D(pool_size=(1,10)))\n",
    "density_localization_model.add(Flatten())\n",
    "density_localization_model.add(Dense(1))\n",
    "density_localization_model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "density_localization_model.compile(optimizer='adam',\n",
    "                               loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(density_localization_model,density_localization_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple motifs (multi-task)<a name='7'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define simulation parameters \n",
    "multi_motif_parameters = {\n",
    "    \"motif_names\": [\"CTCF_known1\",\"ZNF143_known2\",\"SIX5_known1\"],\n",
    "    \"seq_length\": 500,\n",
    "    \"min_num_motifs\": 0,\n",
    "    \"max_num_motifs\": 1, \n",
    "    \"num_seqs\": 20000,\n",
    "    \"GC_fraction\": 0.4}\n",
    "\n",
    "#Get simulation data\n",
    "multi_motif_data=get_simulation_data(\"simulate_multi_motif_embedding\",\n",
    "                               multi_motif_parameters,\n",
    "                               validation_set_size=3200,test_set_size=4000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the optimal model architecture in keras (Refer to Primer Tutorial 2)\n",
    "multi_motif_model=Sequential() \n",
    "multi_motif_model.add(Conv2D(filters=20,kernel_size=(1,20),input_shape=multi_motif_data.X_train.shape[1::]))\n",
    "multi_motif_model.add(Activation('relu'))\n",
    "multi_motif_model.add(MaxPooling2D(pool_size=(1,10)))\n",
    "multi_motif_model.add(Flatten())\n",
    "multi_motif_model.add(Dense(1))\n",
    "multi_motif_model.add(Activation(\"sigmoid\"))\n",
    "\n",
    "##compile the model, specifying the Adam optimizer, and binary cross-entropy loss. \n",
    "multi_motif_model.compile(optimizer='adam',\n",
    "                               loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(multi_motif_model, multi_motif_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heterotypic motifs spatial grammar<a name='8'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define simulation parameters \n",
    "heterodimer_parameters = {\n",
    "    \"motif1\": \"SPI1_known4\",\n",
    "    \"motif2\": \"IRF_known1\",\n",
    "    \"seq_length\": 500,\n",
    "    \"min_spacing\": 2,\n",
    "    \"max_spacing\": 5, \n",
    "    \"num_pos\": 10000,\n",
    "    \"num_neg\": 10000,\n",
    "    \"GC_fraction\": 0.4}\n",
    "\n",
    "#Get simulation data\n",
    "heterodimer_data=get_simulation_data(\"simulate_heterodimer_grammar\",\n",
    "                               heterodimer_parameters,\n",
    "                               validation_set_size=3200,test_set_size=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heterodimer_model=Sequential()\n",
    "heterodimer_model.add(Conv2D(filters=15,kernel_size=(1,15),input_shape=input_shape))\n",
    "heterodimer_model.add(Activation(\"relu\"))\n",
    "heterodimer_model.add(Conv2D(filters=15,kernel_size=(1,15),input_shape=input_shape))\n",
    "heterodimer_model.add(Activation(\"relu\"))\n",
    "heterodimer_model.add(Conv2D(filters=15,kernel_size=(1,15),input_shape=input_shape))\n",
    "heterodimer_model.add(Activation(\"relu\"))\n",
    "heterodimer_model.add(MaxPooling2D(pool_size=(1,35)))    \n",
    "heterodimer_model.add(Flatten())\n",
    "heterodimer_model.add(Dense(num_tasks))\n",
    "heterodimer_model.add(Activation(\"sigmoid\"))\n",
    "heterodimer_model.compile(optimizer='adam',loss='binary_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(heterodimer_model,heterodimer_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions<a name='9'>\n",
    "<a href=#outline>Home</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
